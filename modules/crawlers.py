# modules/crawlers.py
"""
ë‹¤ì–‘í•œ ë³´ì•ˆ ë‰´ìŠ¤ ì›¹ì‚¬ì´íŠ¸ ë° RSS í”¼ë“œë¥¼ í¬ë¡¤ë§í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.
- NCSC ë³´ì•ˆê³µì§€ (Selenium)
- KRCERT ë³´ì•ˆê³µì§€ (RSS)
- ë³´ì•ˆë‰´ìŠ¤ (RSS)
- ë°ì¼ë¦¬ì‹œí (RSS)
"""

import time
import urllib.request
import xml.etree.ElementTree as ET
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta
import urllib.parse # <-- ì´ ë¶€ë¶„ë„ ì¶”ê°€ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

# Selenium ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options

# ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ ë° ì„¤ì •ê°’ ì„í¬íŠ¸
from config import ssl_context, CVE_DATABASE_ID, BOANISSUE_DATABASE_ID
from .utils import date_re, send_slack_message
from .notion_handler import Duplicate_check, create_notion_page, get_recent_entries # get_recent_entries ì¶”ê°€
from .gemini_handler import summarize_text, details_text, CVE_details_text, extract_and_explain_keywords, generate_weekly_tech_blog_post

def crawl_ncsc_page():
    """
    NCSC(êµ­ê°€ì‚¬ì´ë²„ì•ˆë³´ì„¼í„°) ì›¹ì‚¬ì´íŠ¸ì˜ 'ë³´ì•ˆê³µì§€' ê²Œì‹œíŒì—ì„œ ìµœì‹  ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    """
    source_name = "NCSC ë³´ì•ˆê³µì§€"
    driver = None
    print(f"--- {source_name} í¬ë¡¤ë§ ì‹œì‘ ---")
    try:
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36")

        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
        except Exception as e:
            print(f"[{source_name}-ERROR] ChromeDriver ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
            send_slack_message(f"[ERROR] {source_name} - ChromeDriver ì„¤ì • ì˜¤ë¥˜: {e}")
            return

        target_url = "https://www.ncsc.go.kr:4018"
        driver.get(target_url)
        print(f"[{source_name}-INFO] NCSC ì›¹ì‚¬ì´íŠ¸ ì ‘ì† ì‹œë„: {target_url}")

        driver.execute_script("goSubMenuPage('020000','020200')")
        print(f"[{source_name}-INFO] ë³´ì•ˆê³µì§€ ë©”ë‰´ë¡œ ì´ë™ ì‹¤í–‰ (goSubMenuPage('020000','020200'))")

        time.sleep(5)

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        board_list_table = soup.find('table', class_='board_list')
        if not board_list_table:
            message = f"[{source_name}-WARN] ê²Œì‹œíŒ ëª©ë¡ í…Œì´ë¸”('table.board_list')ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            print(message)
            send_slack_message(f"[WARNING] {message} (URL: {driver.current_url})")
            return

        tr_tags = board_list_table.find('tbody').find_all('tr')
        if not tr_tags:
            print(f"[{source_name}-INFO] ê²Œì‹œíŒì— ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"[{source_name}-INFO] {len(tr_tags)}ê°œì˜ ê²Œì‹œê¸€ í–‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ê° í•­ëª© ì²˜ë¦¬ ì‹œì‘...")

        items_processed = 0
        for tr_tag in tr_tags:
            items_processed +=1
            td_tags = tr_tag.find_all('td')
            title_cell = td_tags[1]
            a_tag = title_cell.find('a')

            if a_tag and a_tag.has_attr('onclick'):
                article_title = a_tag.text.strip()

                onclick_script = a_tag['onclick']
                print(f"  [{source_name}-INFO] '{article_title}' ìƒì„¸ í˜ì´ì§€ ì´ë™ ì‹œë„ (onclick: {onclick_script})")
                driver.execute_script(onclick_script)
                time.sleep(3)

                article_url = driver.current_url

                posting_date_str = td_tags[2].text.strip()
                posting_date = date_re(posting_date_str)

                if not posting_date:
                    print(f"  [{source_name}-SKIP] ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ í•­ëª©: {article_title} (ì›ë³¸ ë‚ ì§œ: {posting_date_str})")
                    driver.back()
                    time.sleep(2)
                    continue

                duplicate_status = Duplicate_check(article_url,BOANISSUE_DATABASE_ID)
                if duplicate_status == 0:
                    print(f"  [{source_name}-PROCESSING] ìƒˆ í•­ëª©: {article_title}")

                    # --- NCSC ìƒì„¸ í˜ì´ì§€ ë³¸ë¬¸ í¬ë¡¤ë§ ë¡œì§ ì‹œì‘ ---
                    page_detail_soup = BeautifulSoup(driver.page_source, 'html.parser')
                    article_content_element = page_detail_soup.find('div', class_='board_view_con')

                    page_text_content = ""
                    
                    if article_content_element:
                        # 1. 'editor_view' ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ë¥¼ ìš°ì„  ì‹œë„
                        editor_view_element = article_content_element.find('div', class_='editor_view')
                        if editor_view_element:
                            extracted_text_from_editor = editor_view_element.get_text(separator='\n', strip=True)
                            if extracted_text_from_editor:
                                page_text_content = extracted_text_from_editor
                                print(f"  [{source_name}-INFO] 'editor_view'ì—ì„œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì„±ê³µ. ê¸¸ì´: {len(page_text_content)}ì")
                            else:
                                print(f"  [{source_name}-INFO] 'editor_view'ëŠ” ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ ì •ë³´ í™•ì¸.")
                        else:
                            print(f"  [{source_name}-INFO] 'editor_view'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€/ì²¨ë¶€íŒŒì¼ ì •ë³´ í™•ì¸.")

                        # 2. ì´ë¯¸ì§€ì˜ title ì†ì„±ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„ (page_text_contentê°€ ë¹„ì–´ìˆì„ ê²½ìš° ë˜ëŠ” ì¶”ê°€ ì •ë³´ë¡œ)
                        img_element = article_content_element.find('img')
                        if img_element and img_element.get('title'):
                            img_title = urllib.parse.unquote(img_element['title']).strip()
                            if img_title.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')): # íŒŒì¼ëª… í™•ì¥ì ì œê±°
                                img_title = img_title.rsplit('.', 1)[0]
                            
                            if not page_text_content.strip(): # í…ìŠ¤íŠ¸ ë³¸ë¬¸ì´ ë¹„ì–´ìˆë‹¤ë©´ ì´ë¯¸ì§€ titleë¡œ ëŒ€ì²´
                                page_text_content = f"NCSC ë³´ì•ˆê³µì§€: {article_title} (í•µì‹¬ ë‚´ìš©: {img_title})"
                                print(f"  [{source_name}-INFO] ì´ë¯¸ì§€ ì œëª©({img_title})ìœ¼ë¡œ ë³¸ë¬¸ ë‚´ìš© êµ¬ì„±.")
                            elif img_title not in page_text_content: # ê¸°ì¡´ ë‚´ìš©ì— ì´ë¯¸ì§€ titleì´ ì—†ë‹¤ë©´ ì¶”ê°€ ì •ë³´ë¡œ ë³‘í•©
                                page_text_content += f"\n\ní•µì‹¬ ì´ë¯¸ì§€: {img_title}"
                                print(f"  [{source_name}-INFO] ê¸°ì¡´ ë³¸ë¬¸ì— ì´ë¯¸ì§€ ì œëª©({img_title}) ì¶”ê°€.")

                        # 3. ì²¨ë¶€íŒŒì¼ ì •ë³´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ê°€ ì‹œë„ (page_text_contentê°€ ë¹„ì–´ìˆì„ ê²½ìš° ë˜ëŠ” ì¶”ê°€ ì •ë³´ë¡œ)
                        attachment_file_box = page_detail_soup.find('div', class_='board_view_file')
                        if attachment_file_box:
                            attachment_link = attachment_file_box.find('a', onclick=lambda x: x and 'fn_downFile' in x)
                            if attachment_link:
                                attachment_text = attachment_link.get_text(strip=True)
                                if attachment_text.strip():
                                    if not page_text_content.strip(): # ë³¸ë¬¸ ë‚´ìš©ì´ ì•„ì§ ë¹„ì–´ìˆë‹¤ë©´ ì²¨ë¶€íŒŒì¼ ì œëª©ìœ¼ë¡œ ëŒ€ì²´
                                        page_text_content = f"NCSC ë³´ì•ˆê³µì§€: {article_title} (ì²¨ë¶€íŒŒì¼: {attachment_text})"
                                        print(f"  [{source_name}-INFO] ì²¨ë¶€íŒŒì¼({attachment_text})ë¡œ ë³¸ë¬¸ ë‚´ìš© êµ¬ì„±.")
                                    elif attachment_text not in page_text_content: # ê¸°ì¡´ ë‚´ìš©ì— ì²¨ë¶€íŒŒì¼ ì •ë³´ê°€ ì—†ë‹¤ë©´ ì¶”ê°€
                                        page_text_content += f"\n\nê´€ë ¨ íŒŒì¼: {attachment_text}"
                                        print(f"  [{source_name}-INFO] ê¸°ì¡´ ë³¸ë¬¸ì— ì²¨ë¶€íŒŒì¼({attachment_text}) ì •ë³´ ì¶”ê°€.")
                    else:
                        # board_view_con ìš”ì†Œ ìì²´ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
                        print(f"  [{source_name}-WARN] ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ìš”ì†Œ('div.board_view_con')ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì œëª©ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                        send_slack_message(f"[WARNING] {source_name} '{article_title}' ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨. URL: {article_url}")
                        page_text_content = article_title # ìš”ì†Œ ì°¾ê¸° ì‹¤íŒ¨ ì‹œ ì œëª©ìœ¼ë¡œ ëŒ€ì²´
                    
                    # ìµœì¢…ì ìœ¼ë¡œ page_text_contentê°€ ì—¬ì „íˆ ë¹„ì–´ ìˆë‹¤ë©´, ì›ë˜ì˜ ì„ì‹œ ë¡œì§ ì ìš©
                    if not page_text_content.strip():
                        print(f"  [{source_name}-WARN] ìµœì¢… ë³¸ë¬¸ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆì–´ ì´ˆê¸° ì„ì‹œ ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        summary_for_notion = f"NCSC ë³´ì•ˆê³µì§€: {article_title}"
                        details_for_notion = f"## ğŸ” ë‰´ìŠ¤ ìš”ì•½\n\nNCSC(êµ­ê°€ì‚¬ì´ë²„ì•ˆë³´ì„¼í„°)ì—ì„œ '{article_title}'ì— ëŒ€í•œ ë³´ì•ˆê³µì§€ë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.\n\n## ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸\n\n- ìì„¸í•œ ë‚´ìš©ì€ ì›ë¬¸ ë§í¬ë¥¼ ì°¸ì¡°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
                    else:
                        # í…ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œ/êµ¬ì„±ëœ ê²½ìš° Gemini API í˜¸ì¶œ
                        summarized_content = summarize_text(page_text_content)
                        details_content = details_text(page_text_content)

                        if "ì‹¤íŒ¨" in summarized_content or "ì‹¤íŒ¨" in details_content:
                            send_slack_message(f"[WARN] {source_name} '{article_title}' ì²˜ë¦¬ ì¤‘ Gemini API ì‹¤íŒ¨. "
                                               f"ìš”ì•½: {summarized_content}, ìƒì„¸: {details_content}")
                        
                        summary_for_notion = summarized_content
                        details_for_notion = details_content

                    # --- NCSC ìƒì„¸ í˜ì´ì§€ ë³¸ë¬¸ í¬ë¡¤ë§ ë¡œì§ ë ---

                    create_notion_page(article_title, summary_for_notion, article_url, posting_date, "NCSC", details_for_notion, BOANISSUE_DATABASE_ID)

                elif duplicate_status == 1:
                    print(f"  [{source_name}-SKIP] ì¤‘ë³µëœ í•­ëª©: {article_title}")
                else:
                    print(f"  [{source_name}-SKIP] ì¤‘ë³µ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ í•­ëª©: {article_title}")

                print(f"  [{source_name}-INFO] ëª©ë¡ í˜ì´ì§€ë¡œ ë³µê·€ ì¤‘...")
                driver.back()
                time.sleep(2)
            else:
                print(f"  [{source_name}-WARN] {items_processed}ë²ˆì§¸ í–‰ì—ì„œ ì œëª© ë§í¬(a_tag) ë˜ëŠ” onclick ì†ì„±ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"[{source_name}-ERROR] í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        current_url_info = ""
        if driver and hasattr(driver, 'current_url') and driver.current_url:
            current_url_info = f" (í˜„ì¬ URL: {driver.current_url})"
        send_slack_message(f"[ERROR] {source_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ{current_url_info}: {e}")
    finally:
        if driver:
            driver.quit()
            print(f"[{source_name}-INFO] WebDriver ì¢…ë£Œë¨.")

            
def securityNotice_crawling():
    """KRCERT(í•œêµ­ì¸í„°ë„·ì§„í¥ì›) ë³´ì•ˆ ê³µì§€ RSS í”¼ë“œë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    source_name = "KRCERT ë³´ì•ˆê³µì§€"
    url = 'http://knvd.krcert.or.kr/rss/securityNotice.do'
    try:
        print(f"--- {source_name} í¬ë¡¤ë§ ì‹œì‘ ({url}) ---")
        with urllib.request.urlopen(url, context=ssl_context, timeout=15) as response:
            xml_data = response.read().decode('utf-8')

        root = ET.fromstring(xml_data)
        channel = root.find('channel')
        if channel is None:
            print(f"{source_name} RSS í”¼ë“œì—ì„œ 'channel' íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        items_processed = 0
        for item_elem in channel.findall('item'):
            items_processed +=1
            title = item_elem.findtext('title', default='ì œëª© ì—†ìŒ').strip()
            link_url = item_elem.findtext('link', default='').strip()
            original_content = item_elem.findtext('description', default='ë‚´ìš© ì—†ìŒ').strip()
            pub_date_str = item_elem.findtext('pubDate', default='').strip()
            category_ = "KRCERT"

            if not link_url:
                print(f"[{source_name}-SKIP] URL ì—†ëŠ” í•­ëª©: {title}")
                continue

            posting_date = date_re(pub_date_str)
            if not posting_date:
                print(f"[{source_name}-SKIP] ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ í•­ëª©: {title} (ì›ë³¸ ë‚ ì§œ: {pub_date_str})")
                continue

            duplicate_status = Duplicate_check(link_url,BOANISSUE_DATABASE_ID)
            if duplicate_status == 0:
                print(f"[{source_name}-PROCESSING] ìƒˆ í•­ëª©: {title}")
                summarized_content = summarize_text(original_content)
                details_content = details_text(original_content)

                if "ì‹¤íŒ¨" in summarized_content or "ì‹¤íŒ¨" in details_content:
                    send_slack_message(f"[WARN] {source_name} '{title}' ì²˜ë¦¬ ì¤‘ Gemini API ì‹¤íŒ¨. "
                                       f"ìš”ì•½: {summarized_content}, ìƒì„¸: {details_content}")

                create_notion_page(title, summarized_content, link_url, posting_date, category_, details_content,BOANISSUE_DATABASE_ID)
            elif duplicate_status == 1:
                print(f"[{source_name}-SKIP] ì¤‘ë³µëœ í•­ëª©: {title}")
            else:
                 print(f"[{source_name}-SKIP] ì¤‘ë³µ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ í•­ëª©: {title}")

    except urllib.error.URLError as e:
        print(f"{source_name} RSS URL({url}) ì—´ê¸° ì‹¤íŒ¨: {e}")
        send_slack_message(f"[ERROR] {source_name} RSS URL ì—´ê¸° ì‹¤íŒ¨: {e}")
    except ET.ParseError as e:
        print(f"{source_name} RSS XML íŒŒì‹± ì‹¤íŒ¨: {e}")
        send_slack_message(f"[ERROR] {source_name} RSS XML íŒŒì‹± ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"{source_name} í¬ë¡¤ë§ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        send_slack_message(f"[ERROR] {source_name} í¬ë¡¤ë§ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")


def boanNews_crawling():
    """ë³´ì•ˆë‰´ìŠ¤ RSS í”¼ë“œë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤. pubDateì™€ dc:date íƒœê·¸ë¥¼ ëª¨ë‘ í™•ì¸í•©ë‹ˆë‹¤."""
    source_name = "ë³´ì•ˆë‰´ìŠ¤"
    namespaces = {'dc': 'http://purl.org/dc/elements/1.1/'}

    try:
        urls = [
            'http://www.boannews.com/media/news_rss.xml?skind=5',
            'http://www.boannews.com/media/news_rss.xml?skind=6',
            'http://www.boannews.com/media/news_rss.xml?mkind=1'
        ]
        print(f"--- {source_name} í¬ë¡¤ë§ ì‹œì‘ ---")
        for rss_url in urls:
            print(f"  - {rss_url} ì²˜ë¦¬ ì¤‘...")
            try:
                response = requests.get(rss_url, timeout=15)
                response.raise_for_status()
                response.encoding = response.apparent_encoding
                root = ET.fromstring(response.text)
                channel = root.find('channel')

                if channel is None:
                    print(f"  [{source_name}-WARN] RSS ({rss_url})ì—ì„œ 'channel' íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue

                for item_elem in channel.findall('item'):
                    title = item_elem.findtext('title', default='ì œëª© ì—†ìŒ').strip()
                    link_url = item_elem.findtext('link', default='').strip()
                    original_content = item_elem.findtext('description', default='ë‚´ìš© ì—†ìŒ').strip()
                    category_ = "ë³´ì•ˆë‰´ìŠ¤"

                    pub_date_str = item_elem.findtext('pubDate', default='').strip()
                    if not pub_date_str:
                        dc_date_element = item_elem.find('dc:date', namespaces)
                        if dc_date_element is None:
                            dc_date_element = item_elem.find('{http://purl.org/dc/elements/1.1/}date')
                        if dc_date_element is not None and dc_date_element.text:
                            pub_date_str = dc_date_element.text.strip()

                    if not link_url:
                        print(f"  [{source_name}-SKIP] URL ì—†ëŠ” í•­ëª©: {title}")
                        continue

                    posting_date = date_re(pub_date_str)
                    if not posting_date:
                        print(f"  [{source_name}-SKIP] ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ í•­ëª©: {title} (ì›ë³¸ ë‚ ì§œ: '{pub_date_str}')")
                        continue

                    if rss_url.endswith('mkind=1') and "[ê¸´ê¸‰]" not in title:
                        continue

                    duplicate_status = Duplicate_check(link_url,BOANISSUE_DATABASE_ID)
                    if duplicate_status == 0:
                        print(f"  [{source_name}-PROCESSING] ìƒˆ í•­ëª©: {title}")
                        summarized_content = summarize_text(original_content)
                        details_content = details_text(original_content)
                        if "ì‹¤íŒ¨" in summarized_content or "ì‹¤íŒ¨" in details_content:
                            send_slack_message(f"[WARN] {source_name} '{title}' ì²˜ë¦¬ ì¤‘ Gemini API ì‹¤íŒ¨. "
                                               f"ìš”ì•½: {summarized_content}, ìƒì„¸: {details_content}")
                        create_notion_page(title, summarized_content, link_url, posting_date, category_, details_content,BOANISSUE_DATABASE_ID)
                    elif duplicate_status == 1:
                        print(f"  [{source_name}-SKIP] ì¤‘ë³µëœ í•­ëª©: {title}")
                    else:
                        print(f"  [{source_name}-SKIP] ì¤‘ë³µ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ í•­ëª©: {title}")

            except requests.exceptions.RequestException as e:
                print(f"  [{source_name}-ERROR] RSS ({rss_url}) ìš”ì²­ ì‹¤íŒ¨: {e}")
                send_slack_message(f"[ERROR] {source_name} RSS ({rss_url}) ìš”ì²­ ì‹¤íŒ¨: {e}")
                continue
            except ET.ParseError as e:
                print(f"  [{source_name}-ERROR] RSS ({rss_url}) XML íŒŒì‹± ì‹¤íŒ¨: {e}")
                send_slack_message(f"[ERROR] {source_name} RSS ({rss_url}) XML íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
    except Exception as e:
        print(f"{source_name} í¬ë¡¤ë§ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        send_slack_message(f"[ERROR] {source_name} í¬ë¡¤ë§ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")


def dailysecu_crawling():
    """ë°ì¼ë¦¬ì‹œí RSS í”¼ë“œë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    source_name = "ë°ì¼ë¦¬ì‹œí"
    url = 'https://www.dailysecu.com/rss/S1N2.xml'
    try:
        print(f"--- {source_name} í¬ë¡¤ë§ ì‹œì‘ ({url}) ---")
        try:
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            root = ET.fromstring(response.text)
            channel = root.find('channel')
            if channel is None:
                print(f"{source_name} RSS ({url})ì—ì„œ 'channel' íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

            for item_elem in channel.findall('item'):
                title = item_elem.findtext('title', default='ì œëª© ì—†ìŒ').strip()
                link_url = item_elem.findtext('link', default='').strip()
                original_content = item_elem.findtext('description', default='ë‚´ìš© ì—†ìŒ').strip()
                category_ = "ë°ì¼ë¦¬ì‹œí"

                pub_date_str = item_elem.findtext('pubDate', default='').strip()
                if not pub_date_str:
                    dc_date_elem = item_elem.find('{http://purl.org/dc/elements/1.1/}date')
                    if dc_date_elem is not None and dc_date_elem.text:
                        pub_date_str = dc_date_elem.text.strip()

                if not link_url:
                    print(f"[{source_name}-SKIP] URL ì—†ëŠ” í•­ëª©: {title}")
                    continue

                posting_date = date_re(pub_date_str)
                if not posting_date:
                    print(f"[{source_name}-SKIP] ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ í•­ëª©: {title} (ì›ë³¸ ë‚ ì§œ: {pub_date_str})")
                    continue

                duplicate_status = Duplicate_check(link_url,BOANISSUE_DATABASE_ID)
                if duplicate_status == 0:
                    print(f"[{source_name}-PROCESSING] ìƒˆ í•­ëª©: {title}")
                    summarized_content = summarize_text(original_content)
                    details_content = details_text(original_content)
                    if "ì‹¤íŒ¨" in summarized_content or "ì‹¤íŒ¨" in details_content:
                        send_slack_message(f"[WARN] {source_name} '{title}' ì²˜ë¦¬ ì¤‘ Gemini API ì‹¤íŒ¨. "
                                           f"ìš”ì•½: {summarized_content}, ìƒì„¸: {details_content}")
                    create_notion_page(title, summarized_content, link_url, posting_date, category_, details_content,BOANISSUE_DATABASE_ID)
                elif duplicate_status == 1:
                    print(f"[{source_name}-SKIP] ì¤‘ë³µëœ í•­ëª©: {title}")
                else:
                    print(f"[{source_name}-SKIP] ì¤‘ë³µ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ í•­ëª©: {title}")

        except requests.exceptions.RequestException as e:
            print(f"{source_name} RSS ({url}) ìš”ì²­ ì‹¤íŒ¨: {e}")
            send_slack_message(f"[ERROR] {source_name} RSS ({url}) ìš”ì²­ ì‹¤íŒ¨: {e}")
        except ET.ParseError as e:
            print(f"{source_name} RSS ({url}) XML íŒŒì‹± ì‹¤íŒ¨: {e}")
            send_slack_message(f"[ERROR] {source_name} RSS ({url}) XML íŒŒì‹± ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"{source_name} í¬ë¡¤ë§ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        send_slack_message(f"[ERROR] {source_name} í¬ë¡¤ë§ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")

def nvd_cve_crawling():
    """NVD API v2.0 JSON ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤ (ìµœê·¼ 90ì¼ë§Œ)."""
    source_name = "NVD CVE"
    base_url = 'https://services.nvd.nist.gov/rest/json/cves/2.0'

    end_date = datetime.utcnow()
    start_date = end_date - timedelta(days=10) # NVD APIëŠ” ìµœëŒ€ 120ì¼
    pubStartDate = start_date.strftime("%Y-%m-%dT00:00:00.000")
    pubEndDate = end_date.strftime("%Y-%m-%dT23:59:59.999")

    params = {
        "pubStartDate": pubStartDate,
        "pubEndDate": pubEndDate,
        "resultsPerPage": 2000
    }

    try:
        print(f"--- {source_name} í¬ë¡¤ë§ ì‹œì‘ ({base_url}) ---")
        try:
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            vulnerabilities = data.get('vulnerabilities', [])
            for vuln in vulnerabilities:
                cve = vuln.get('cve', {})
                cve_id = cve.get('id', 'ì œëª© ì—†ìŒ')
                published = cve.get('published', '')
                descriptions = cve.get('descriptions', [])
                description_en = next((desc['value'] for desc in descriptions if desc['lang'] == 'en'), 'ë‚´ìš© ì—†ìŒ')

                category_ = "CVE"
                link_url = f"https://nvd.nist.gov/vuln/detail/{cve_id}"

                posting_date = date_re(published)

                duplicate_status = Duplicate_check(link_url, CVE_DATABASE_ID)
                if duplicate_status == 0:
                    print(f"[{source_name}-PROCESSING] ìƒˆ í•­ëª©: {cve_id}")
                    # CVE_details_textê°€ ì´ì œ (ì œëª©, ë³¸ë¬¸) íŠœí”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤.
                    generated_cve_title, generated_cve_body = CVE_details_text(description_en)
                    
                    # ìš”ì•½ì€ ë³„ë„ë¡œ summarize_textë¡œ ë§Œë“¤ê±°ë‚˜, generated_cve_bodyì—ì„œ ì¼ë¶€ ë°œì·Œ
                    summarized_content = summarize_text(description_en) # ì›ë˜ëŒ€ë¡œ ì›ë¬¸ìœ¼ë¡œ ìš”ì•½

                    if "ì‹¤íŒ¨" in generated_cve_title or "ì‹¤íŒ¨" in generated_cve_body:
                        send_slack_message(f"[WARN] {source_name} '{cve_id}' ì²˜ë¦¬ ì¤‘ Gemini API ì‹¤íŒ¨. "
                                           f"ìƒì„± ì œëª©: {generated_cve_title}, ìƒì„± ë³¸ë¬¸: {generated_cve_body}")

                    # Notion í˜ì´ì§€ ìƒì„± ì‹œ, Geminiê°€ ìƒì„±í•œ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
                    create_notion_page(generated_cve_title, summarized_content, link_url, posting_date, category_, generated_cve_body, CVE_DATABASE_ID)
                elif duplicate_status == 1:
                    print(f"[{source_name}-SKIP] ì¤‘ë³µëœ í•­ëª©: {cve_id}")
                else:
                    print(f"[{source_name}-SKIP] ì¤‘ë³µ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ í•­ëª©: {cve_id}")

        except requests.exceptions.RequestException as e:
            print(f"{source_name} API ìš”ì²­ ì‹¤íŒ¨: {e}")
            send_slack_message(f"[ERROR] {source_name} API ìš”ì²­ ì‹¤íŒ¨: {e}")
        except json.JSONDecodeError as e:
            print(f"{source_name} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            send_slack_message(f"[ERROR] {source_name} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"{source_name} í¬ë¡¤ë§ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        send_slack_message(f"[ERROR] {source_name} í¬ë¡¤ë§ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")


# Week_nvd_cve_crawling í•¨ìˆ˜ ë‚´ë¶€ ìˆ˜ì •:
def Week_nvd_cve_crawling():
    """NVD API v2.0 JSON ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ìµœê·¼ 7ì¼ì¹˜ CVEë¥¼ í•˜ë‚˜ì˜ Notion í˜ì´ì§€ë¡œ ë“±ë¡í•©ë‹ˆë‹¤."""
    source_name = "NVD CVE ì£¼ê°„ ìš”ì•½" # ì´ë¦„ ë³€ê²½ìœ¼ë¡œ ëª…í™•í™”
    base_url = 'https://services.nvd.nist.gov/rest/json/cves/2.0'

    # ìµœê·¼ 7ì¼ êµ¬ê°„ ê³„ì‚°
    end_date = datetime.utcnow()
    start_date = end_date - timedelta(days=7)
    pubStartDate = start_date.strftime("%Y-%m-%dT00:00:00.000")
    pubEndDate = end_date.strftime("%Y-%m-%dT23:59:59.999")

    params = {
        "pubStartDate": pubStartDate,
        "pubEndDate": pubEndDate,
        "resultsPerPage": 2000
    }

    try:
        print(f"--- {source_name} í¬ë¡¤ë§ ì‹œì‘ ({base_url}) ---")
        try:
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            vulnerabilities = data.get('vulnerabilities', [])
            all_descriptions_for_summary = []  # ëª¨ë“  CVE ì›ë¬¸ì„ ìš”ì•½ìš©ìœ¼ë¡œ ëª¨ì„ ë¦¬ìŠ¤íŠ¸

            for vuln in vulnerabilities:
                cve = vuln.get('cve', {})
                cve_id = cve.get('id', 'ì œëª© ì—†ìŒ')
                descriptions = cve.get('descriptions', [])
                description_en = next((desc['value'] for desc in descriptions if desc['lang'] == 'en'), 'ë‚´ìš© ì—†ìŒ')
                
                # ì£¼ê°„ ìš”ì•½ìš© í…ìŠ¤íŠ¸ì— ê° CVE IDì™€ ì„¤ëª…ì„ ì¶”ê°€
                all_descriptions_for_summary.append(f"CVE ID: {cve_id}\nì„¤ëª…: {description_en}\n")

            # ëª¨ë“  CVE ì›ë¬¸ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
            combined_cve_text = "\n---\n".join(all_descriptions_for_summary)

            if all_descriptions_for_summary:
                print(f"ì´ {len(vulnerabilities)}ê°œì˜ CVEë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ìš”ì²­ ì¤‘...")
                # CVE_details_text í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ (ì œëª©, ë³¸ë¬¸) íŠœí”Œì„ ë°›ìŠµë‹ˆë‹¤.
                raw_generated_blog_title, generated_blog_body = CVE_details_text(combined_cve_text)
                
                # Notion ì†ì„±ìš© ìš”ì•½ì€ ë³„ë„ë¡œ ìƒì„± (ì˜ˆ: ì²˜ìŒ 200ì)
                page_summary_for_notion_property = "ìµœê·¼ 7ì¼ê°„ì˜ ì£¼ìš” CVEë¥¼ ë¶„ì„í•œ ìƒì„¸ ë³´ê³ ì„œì…ë‹ˆë‹¤."
                if len(generated_blog_body) > 200:
                    page_summary_for_notion_property = generated_blog_body[:197] + "..."

                if "ì‹¤íŒ¨" in raw_generated_blog_title or "ì‹¤íŒ¨" in generated_blog_body:
                    print(f"[{source_name}-ERROR] Gemini APIë¥¼ í†µí•œ CVE ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: ì œëª©: {raw_generated_blog_title}, ë³¸ë¬¸: {generated_blog_body}")
                    send_slack_message(f"[ERROR] {source_name} - CVE ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {raw_generated_blog_title}, {generated_blog_body}")
                    return
                
                # Geminiê°€ ìƒì„±í•œ ì œëª© ì•ì— ê¸°ê°„ì„ ë¶™ì…ë‹ˆë‹¤.
                generated_blog_title = f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')} {raw_generated_blog_title}"

            else:
                # CVEê°€ ì—†ì„ ê²½ìš°ì—ë„ ê¸°ê°„ì„ í¬í•¨í•œ ì œëª©ì„ ìƒì„±í•©ë‹ˆë‹¤.
                generated_blog_title = f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')} ì£¼ê°„ CVE ìš”ì•½ (ë‚´ìš© ì—†ìŒ)"
                generated_blog_body = "ìµœê·¼ 7ì¼ê°„ ìƒˆë¡œìš´ CVEê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                page_summary_for_notion_property = "ìµœê·¼ 7ì¼ê°„ ìƒˆë¡œìš´ CVEê°€ ì—†ìŠµë‹ˆë‹¤."

            # 7ì¼ì¹˜ CVE ë¸”ë¡œê·¸ë¥¼ í•˜ë‚˜ì˜ í˜ì´ì§€ë¡œ ë“±ë¡
            print(f"[{source_name}] CVE ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ Notion í˜ì´ì§€ë¡œ ë“±ë¡ ì‹œì‘")
            create_notion_page(
                title=generated_blog_title, # ê¸°ê°„ì´ í¬í•¨ëœ ì œëª© ì‚¬ìš©
                content=page_summary_for_notion_property, # Notion ì†ì„±ìš© ìš”ì•½
                url="https://rebugui.tistory.com", # ê³ ì • URL ë˜ëŠ” ì ì ˆíˆ ë³€ê²½
                date=end_date.strftime('%Y-%m-%d'),
                category_="CVE ì£¼ê°„ì´ìŠˆ", # ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬ ë˜ëŠ” ì ì ˆíˆ ë³€ê²½
                details=generated_blog_body, # Geminiê°€ ìƒì„±í•œ ì „ì²´ ë¸”ë¡œê·¸ ë³¸ë¬¸
                DATABASE_ID=CVE_DATABASE_ID # CVE ë°ì´í„°ë² ì´ìŠ¤ ID ì‚¬ìš©
            )
            print(f"[{source_name}] CVE ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ Notion í˜ì´ì§€ ë“±ë¡ ì™„ë£Œ.")
            send_slack_message(f"[INFO] {source_name} - '{generated_blog_title}' Notion í˜ì´ì§€ ìƒì„± ì™„ë£Œ.")
        
        except requests.exceptions.RequestException as e:
            print(f"{source_name} API ìš”ì²­ ì‹¤íŒ¨: {e}")
            send_slack_message(f"[ERROR] {source_name} API ìš”ì²­ ì‹¤íŒ¨: {e}")
        except json.JSONDecodeError as e:
            print(f"{source_name} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            send_slack_message(f"[ERROR] {source_name} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"{source_name} í¬ë¡¤ë§ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        send_slack_message(f"[ERROR] {source_name} í¬ë¡¤ë§ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")


def generate_weekly_tech_keywords():
    """
    ì§€ì •ëœ Notion ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìµœê·¼ 7ì¼ê°„ì˜ ëª¨ë“  ê¸°ìˆ  ê´€ë ¨ ë‚´ìš©ì„ ê°€ì ¸ì™€
    ì£¼ìš” ê¸°ìˆ  í‚¤ì›Œë“œ 10ê°œì™€ ì„¤ëª…ì„ Gemini APIë¥¼ í†µí•´ ìƒì„±í•˜ê³  Notionì— ìƒˆ í˜ì´ì§€ë¡œ ë°œí–‰í•©ë‹ˆë‹¤.
    -> ì´ í•¨ìˆ˜ë¥¼ í™•ì¥í•˜ì—¬ ë¸”ë¡œê·¸ ê¸€ í˜•ì‹ìœ¼ë¡œ ì£¼ê°„ ì´ìŠˆë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    source_name = "ì£¼ê°„ ê¸°ìˆ  ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„±"
    print(f"--- {source_name} ì‘ì—… ì‹œì‘ (ëŒ€ìƒ DB: {BOANISSUE_DATABASE_ID}) ---")

    # 1. Notion DBì—ì„œ ìµœê·¼ 7ì¼ê°„ì˜ ëª¨ë“  í•­ëª© ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    # ì´ í…ìŠ¤íŠ¸ëŠ” Gemini APIì˜ 'ì£¼ì œ'ì´ì 'ë³¸ë¬¸'ìœ¼ë¡œ í™œìš©ë©ë‹ˆë‹¤.
    combined_articles_text = get_recent_entries(BOANISSUE_DATABASE_ID)

    if not combined_articles_text:
        print(f"[{source_name}] ìµœê·¼ 7ì¼ê°„ì˜ ê¸°ìˆ  ì •ë³´ê°€ ì—†ì–´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        send_slack_message(f"[INFO] {source_name} - ìµœê·¼ 7ì¼ê°„ ê¸°ìˆ  ì •ë³´ ì—†ìŒ. ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ê±´ë„ˆëœ€.")
        return

    # 2. Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¸”ë¡œê·¸ ê¸€ ìƒì„±
    # combined_articles_textë¥¼ {ì£¼ì œ}ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
    print(f"[{source_name}] ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ ({len(combined_articles_text)}ì)ë¡œ ê¸°ìˆ  ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ìš”ì²­ ì¤‘...")
    
    # generate_weekly_tech_blog_post í•¨ìˆ˜ëŠ” ì œëª©ê³¼ ë³¸ë¬¸ì„ íŠœí”Œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    generated_title, generated_body = generate_weekly_tech_blog_post(combined_articles_text)

    if "ì‹¤íŒ¨" in generated_title or "ì‹¤íŒ¨" in generated_body:
        print(f"[{source_name}-ERROR] Gemini APIë¥¼ í†µí•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: ì œëª©: {generated_title}, ë³¸ë¬¸: {generated_body}")
        send_slack_message(f"[ERROR] {source_name} - ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {generated_title}, {generated_body}")
        return

    # 3. Notionì— ìƒˆë¡œìš´ í˜ì´ì§€ë¡œ ë°œí–‰
    current_date = datetime.now().strftime('%Y-%m-%d')
    start_date = (datetime.now() - timedelta(days=6)).strftime('%Y-%m-%d') # ìµœê·¼ 7ì¼ì´ë¯€ë¡œ ì˜¤ëŠ˜ í¬í•¨ 7ì¼ ì „

    # generate_weekly_tech_blog_postì—ì„œ ë°›ì€ ì œëª©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    page_title = generated_title 
    page_summary = "ìµœê·¼ í•œ ì£¼ê°„ ì£¼ìš” ë³´ì•ˆ ë° ê¸°ìˆ  ì´ìŠˆë¥¼ ë¶„ì„í•˜ì—¬ ìƒì„±ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤." # Notio n ì†ì„±ìš© ìš”ì•½
    page_url = "https://rebugui.tistory.com" # í˜¹ì€ ì ì ˆí•œ ê¸°ë³¸ URL ì„¤ì •
    category = "ì£¼ê°„ì´ìŠˆ" # Notion DBì˜ ì¹´í…Œê³ ë¦¬ ì†ì„± ì´ë¦„ì— ë§ê²Œ ì„¤ì •

    print(f"[{source_name}] ìƒì„±ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ Notion í˜ì´ì§€ë¡œ ë°œí–‰: '{page_title}'")
    create_notion_page(
        title=page_title,
        content=page_summary, # Notion ì†ì„±ìš© ìš”ì•½ (ê°„ëµí™”ëœ ë‚´ìš©)
        url=page_url,
        date=current_date,
        category_=category,
        details=generated_body, # Geminiê°€ ìƒì„±í•œ ì „ì²´ ë¸”ë¡œê·¸ ë³¸ë¬¸
        DATABASE_ID=BOANISSUE_DATABASE_ID # ë³´ì•ˆì´ìŠˆë¥¼ ê´€ë¦¬í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ID ì‚¬ìš©
    )
    send_slack_message(f"[INFO] {source_name} - '{page_title}' Notion í˜ì´ì§€ ìƒì„± ì™„ë£Œ.")
    print(f"--- {source_name} ì‘ì—… ì™„ë£Œ ---")
